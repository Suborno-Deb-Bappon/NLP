{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/raqueeb/nlp_bangla/blob/master/text_sequence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DyPZfmyDBOyw"
   },
   "source": [
    "## 'বাংলা' ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এ \"টেক্সট থেকে সিকোয়েন্সে\"\n",
    "\n",
    "## ৩য় চ্যাপ্টার\n",
    "আমরা গত নোটবুকে একটা বাক্যকে কিভাবে শব্দে টোকেনাইজ করতে হয় সেটা দেখেছিলাম। সেটা আমরা করেছিলাম টেন্সরফ্লো এর কয়েকটি টুল দিয়ে। বিশেষ করে fit_on_texts দিয়ে। এবার আলাপ হবে texts_to_sequences মেথড এর ব্যবহার নিয়ে, যাতে পুরো বাক্যকে একটা টেক্সট এর সিকোয়েন্সে ফেলে দেয়া যায়।  ![alt text](https://github.com/raqueeb/nlp_bangla/raw/master/assets/seq1.png)\n",
    "\n",
    "ছবি: ওয়ার্ড ইনডেক্স\n",
    "\n",
    "## স্টপওয়ার্ড (বহুল ব্যবহৃত শব্দ), যতিচিহ্নের কি হবে?\n",
    "\n",
    "আমাদের বাক্যে যেহেতু আমরা দুটো জিনিস নিয়ে কাজ করেছিলাম,  সেখানে আমরা যদি আলাদা করে নতুন আরেকটা লাইন ঢুকাই তখন কি হতে পারে? ধরা যাক আমরা নতুন একটা বাক্য এখানে যোগ করতে চাচ্ছি “বইমেলা এলে আমরা প্রচুর বই কিনি”। এর আগের উদাহরণে আমরা বাংলা দাড়ি ব্যাপারটা ফিল্টারে ঢুকিয়েছিলাম তবে এবার আমরা আমরা নতুন করে বলছি না। দেখি এবার আমাদের টোকেনাইজার কি করে? এখানে আমাদের নতুন কর্পাসে যতি চিহ্ন যোগ করলেও টোকেনাইজার আমাদের যতিচিহ্ন ফেলে দিয়েছে। কারণ বাই ডিফল্ট কেরাসে প্রচুর যতিচিহ্ন যোগ করা আছে এখানে। বাংলার জন্য । (দাড়ি) যোগ করে নিতে হবে। সচরাচর বাংলা স্টপ ওয়ার্ড (আমি, আপনি, এবং, ইত্যাদি ...) নিয়ে আলাপ করবো সামনে। এখানে টোকেনাইজারের গেট_কনফিগ দিয়ে সেটা দেখা যাবে। নিচের উদাহরণে দেখুন।\n",
    "\n",
    "## নতুন শব্দগুলোর কি হলো?\n",
    "\n",
    "সবচেয়ে মজার কথা হচ্ছে আমাদের কর্পাসে নতুন শব্দ যোগ হয়েছে নতুন সংখ্যা জোড়া ভ্যালু সহ। তবে আমরা যাই যোগ করি, শুধুমাত্র নতুন ইউনিক শব্দের একটা করে সিকোয়েন্স নম্বর বসবে। \"আমি\" শব্দটা প্রতি বাক্যে আলাদা করে আসলেও কর্পাসে একবারই আসবে।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FBqUseMdA_CQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'আমি ভালবাসি বই পড়তে,',\n",
    "    'আমি ভালবাসি বই লিখতে!',\n",
    "    'বইমেলা এলে আমি প্রচুর বই কিনি'\n",
    "        ]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYCw5EXgCYj4"
   },
   "source": [
    "আগের মতো \"ফিট অন টেক্সট\" করি।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TbeZpO4WCeXl",
    "outputId": "87f839ea-96e7-4190-9701-04ed951606f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'আমি': 1, 'বই': 2, 'ভালবাসি': 3, 'পড়তে': 4, 'লিখতে': 5, 'বইমেলা': 6, 'এলে': 7, 'প্রচুর': 8, 'কিনি': 9}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZc2et6YmwMx"
   },
   "source": [
    "এবার যতিচিহ্ন অথবা স্টপওয়ার্ড যেহেতু দেইনি আলাদা করে, দেখি টোকেনাইজার কি করে?\n",
    "\n",
    "দেখুন ফিল্টারে, আগে থেকেই দেয়া আছে এই স্টপওয়ার্ড, মানে যতিচিহ্ন, তবে বাংলা দাড়ি (।) যোগ করে নিতে হবে।\n",
    "\n",
    "`\"filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_{|}~\\t\\n',\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "NI5YvqIPnS5m",
    "outputId": "4cb8d1d1-ec23-46bd-d97b-5a3f6f4ce099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_level': False,\n",
       " 'document_count': 3,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'index_docs': '{\"1\": 3, \"2\": 3, \"3\": 2, \"4\": 1, \"5\": 1, \"6\": 1, \"7\": 1, \"9\": 1, \"8\": 1}',\n",
       " 'index_word': '{\"1\": \"\\\\u0986\\\\u09ae\\\\u09bf\", \"2\": \"\\\\u09ac\\\\u0987\", \"3\": \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\", \"4\": \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\", \"5\": \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\", \"6\": \"\\\\u09ac\\\\u0987\\\\u09ae\\\\u09c7\\\\u09b2\\\\u09be\", \"7\": \"\\\\u098f\\\\u09b2\\\\u09c7\", \"8\": \"\\\\u09aa\\\\u09cd\\\\u09b0\\\\u099a\\\\u09c1\\\\u09b0\", \"9\": \"\\\\u0995\\\\u09bf\\\\u09a8\\\\u09bf\"}',\n",
       " 'lower': True,\n",
       " 'num_words': 10,\n",
       " 'oov_token': None,\n",
       " 'split': ' ',\n",
       " 'word_counts': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 3, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09ac\\\\u0987\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 1, \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\": 1, \"\\\\u09ac\\\\u0987\\\\u09ae\\\\u09c7\\\\u09b2\\\\u09be\": 1, \"\\\\u098f\\\\u09b2\\\\u09c7\": 1, \"\\\\u09aa\\\\u09cd\\\\u09b0\\\\u099a\\\\u09c1\\\\u09b0\": 1, \"\\\\u0995\\\\u09bf\\\\u09a8\\\\u09bf\": 1}',\n",
       " 'word_docs': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 3, \"\\\\u09ac\\\\u0987\": 3, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 1, \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\": 1, \"\\\\u09ac\\\\u0987\\\\u09ae\\\\u09c7\\\\u09b2\\\\u09be\": 1, \"\\\\u098f\\\\u09b2\\\\u09c7\": 1, \"\\\\u0995\\\\u09bf\\\\u09a8\\\\u09bf\": 1, \"\\\\u09aa\\\\u09cd\\\\u09b0\\\\u099a\\\\u09c1\\\\u09b0\": 1}',\n",
       " 'word_index': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 1, \"\\\\u09ac\\\\u0987\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 4, \"\\\\u09b2\\\\u09bf\\\\u0996\\\\u09a4\\\\u09c7\": 5, \"\\\\u09ac\\\\u0987\\\\u09ae\\\\u09c7\\\\u09b2\\\\u09be\": 6, \"\\\\u098f\\\\u09b2\\\\u09c7\": 7, \"\\\\u09aa\\\\u09cd\\\\u09b0\\\\u099a\\\\u09c1\\\\u09b0\": 8, \"\\\\u0995\\\\u09bf\\\\u09a8\\\\u09bf\": 9}'}"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vtUdoaqBG8MJ"
   },
   "source": [
    "## আচ্ছা, আমরা কি মেপে কথা বলি?\n",
    "\n",
    "কেমন হতো প্রতিটা বাক্যে একটা নির্দিষ্ট শব্দ থাকতো? সেটা কি কখনো হয়? তাহলে নিউরাল নেটওয়ার্ককে কিভাবে কাজ করাবো? আগে আলাপ করেছিলাম - প্যাডিং।\n",
    "\n",
    "## সমস্যাটা দেখি বিভিন্ন লেনথের বাক্যে\n",
    "\n",
    "আমাদের আগের বাক্যে চারটা করে শব্দ ছিল। এখন নতুন একটা বাক্য যোগ করলাম যেখানে ৬টা শব্দ যোগ করা হয়েছে। ইনডেক্সে নতুন বাক্যের ভেতরে ইউনিক শব্দগুলো বাদ দিয়ে বাকি শব্দগুলোকে নতুন করে জোড়া ‘কি-ভ্যালু’ দিয়ে দিয়েছে। এটা একটা নতুন ওয়ার্ড ভ্যালু ইনডেক্স যোগ করল কর্পাসে। দেখতে পাচ্ছেন তো ছবিতে?\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/raqueeb/nlp_bangla/master/assets/seq2.png)\n",
    "\n",
    "ছবি: নতুন ওয়ার্ড ইনডেক্স তৈরি হয়েছে কর্পাসে\n",
    "\n",
    "এখানে আমরা একটা বাক্যে বিভিন্ন শব্দ দিয়ে যে ডিকশনারি তৈরি করলাম সেটাকে কর্পাস হিসেবে ব্যবহার করছি। এতক্ষণ আমরা যা খেলেছি সেগুলো হচ্ছে শব্দকে এনকোড করা। এখন আমরা আমাদের বাক্যগুলোকে একটা লিস্টে ভাগ করব যেখানে টোকেনগুলোকে পাশাপাশি দেখা যাবে। এরপর টোকেনগুলোকে কিভাবে এদিক ওদিক করা যায় সেটা দেখবো আমরা।\n",
    "\n",
    "এখানে একটা মজার ধারণা নিয়ে আলাপ করি।\n",
    "\n",
    "## প্যাডিং, একই লেনথ এর বাক্য?\n",
    "\n",
    "আমরা এ পর্যন্ত যতগুলো বাক্য বলেছি তার মধ্যে দুটো বাক্য চারটা শব্দের আর একটা বাক্য ছয়টা শব্দের। আমরা যখন কোন নিউরাল নেটওয়ার্ক কিছু ফিট করতে চাই তখন সবকিছুই একি লেংথ বা সাইজ করে পাঠাই। মনে আছে আমরা ইমেজ নিয়ে কিভাবে কাজ করেছিলাম?  আমাদের ইনপুট লেয়ারে যখন প্রতিটা ইমেজ ফিড করছিলাম নিউরাল নেটওয়ার্ক সেখানে যদি ইমেজগুলো আলাদা সাইজের হতো সেগুলো কে প্রথমে আমরা রিসাইজ করে নিতাম  এক সাইজে। আমরা যখন টেক্সট নিয়ে কাজ করব তখন আমরা কখনোই বলতে পারব না একটা বাক্যে কতগুলো শব্দ হতে পারে এবং সেগুলোর সংখ্যা অনেকটাই অজানা থাকবে একটা নিউরাল নেটওয়ার্কের জন্য। \n",
    "\n",
    "গতবছর আমি scikit-learn দিয়ে ন্যাচারাল ল্যাংগুয়েজ প্রসেসিং ব্যাপারটা শুরু করলেও সেটাকে পরে বন্ধ করে টেন্সরফ্লো দিয়ে শুরু করেছিলাম।  তারপর পেছনে আর ফিরে তাকাতে হয়নি।  টেন্সরফ্লো ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং এর জন্য অনেকগুলো এপিআই তৈরি করে দিয়েছে যা সাধারণ পাইথন এবং scikit-learn দিয়ে ন্যাচারাল ল্যাঙ্গুয়েজ প্রসেসিং থেকে অনেক কষ্ট কমিয়ে নিয়ে এসেছে।  এর আগে আমরা এই জিনিসটার কিছুটা কাজ দেখেছি টেন্সরফ্লো দিয়ে।  আমরা যেহেতু সিকুয়েন্সের একটা লিস্ট ধরে কাজ শুরু করেছিলাম,  সেখানে বাক্যগুলোকে টোকেন দিয়ে এনকোড করে তার পাশাপাশি দরকারি কোড কোথায় কোথায় কিভাবে কাজ করছে সেটাও দেখেছি আগে। \n",
    "\n",
    "যেহেতু আমরা আরেকটা বাক্য যোগ করেছি আমাদের আগের বাক্যের লিস্ট এ,  সেখানে আগের দুটো বাক্যে চারটা শব্দ আর পরের বাক্যে আমরা ছয় শব্দ সহ নতুন বাক্য যোগ করেছিলাম। \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hq83aorjIS5"
   },
   "source": [
    "## ৪টা বিভিন্ন লেনথের বাক্য দিয়ে কর্পাস\n",
    "\n",
    "এখন আমরা দেখতে চাইবো এরকম ভিন্ন ভিন্ন শব্দ সম্বলিত বাক্য গুলোকে কিভাবে আমরা নিউরাল নেটওয়ার্ক পাঠাবো?  আগের ইমেজের মত বলতে গেলে এখানেও আমরা একটা বাক্যে সর্বোচ্চ কতগুলো শব্দ হতে পারে তার পাশাপাশি ছোট শব্দ গুলোকে কিভাবে গোঁজামিল দিয়ে বড় শব্দ বানাবো সেটাই দেখব সামনে। \n",
    "\n",
    "ছোটবেলায় ক্যাডেট কলেজে যখন আমাদেরকে জুতা বানিয়ে দিত,  তখন আমরা ভাবতাম যাতে আমাদের জুতোগুলো একটু বড় থাকে।  কারণ  জুতো ছোট হলে পড়তে কষ্ট, তবে বড় হলে সমস্যা কম কারণ জুতোর ভেতরে কাপড়ের প্যাডিং দিয়ে বড় জুতা কে পড়া যায় সহজে।  জুতো বড় হলে সেখানে একটা মোজার জায়গায় দুটো পড়ে সেই কাজটা চালানো যেত সহজে। \n",
    "\n",
    "## প্রতিটা বাক্যকে সংখ্যার একটা লিস্ট বানাবো টোকেনের ভিত্তিতে\n",
    "\n",
    "এবার আমরা চেষ্টা করব দ্বিতীয় পর্যায়ে যেতে। এখানে texts_to_sequences মেথড দিয়ে আমরা একটা বাক্য থেকে সংখ্যার একটা সিকুয়েন্স তৈরি করব যাতে একটা নিউরাল নেটওয়ার্ককে ঠিকমতো শেখাতে পারে। শুরুতে বাক্য থেকে সিকোয়েন্সের একটা লিস্ট - যা আমরা পেয়েছি শব্দগুলোকে এনকোড করে - তবে এখন এটা আমরা পেয়েছি texts_to_sequences মেথড ব্যবহার করে। এখানে এই টুলটার ব্যবহার দেখুন যাতে সে এই প্রসেসে সংখ্যার একটা সিকোয়েন্স এর লিস্ট তৈরি করতে পারে। আমাদের এই প্যাটার্ন স্পটিংটা খুব মজার।\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "এর মানে হচ্ছে আমাদের কর্পাস sentencesকে পাঠিয়ে দিলাম texts_to_sequences মেথডে, যাতে sequences এর একটা লিস্ট দেখাতে পারে। sequences প্রিন্ট করেই দেখুন না?\n",
    "\n",
    "এই একই জিনিস ঘটবে আমাদের ছোট ছোট বাক্যের জন্য।  আমরা ধরে নিবো একটা বাক্য কত লম্বা হতে পারে।  এর পাশাপাশি আমরা ছোট ছোট বাক্যগুলোকে শূন্য দিয়ে প্যাডিং করে বড় বাক্য বানাবো যাতে  নিউরাল নেটওয়ার্ক সবসময় একই লম্বার বাক্য পায়। চলুন আমরা দেখে আসি আমাদের নতুন কোড,  যেখানে আমরা টোকেনাইজার ব্যবহার করব যাতে আমাদের বাক্যগুলোকে সিকোয়েন্সে পাল্টে দিতে পারে।  আমরা বলতে পারি একেকটা বাক্যের জন্য একেকটা সিকোয়েন্স,  এবং সেখানে প্রতিটা বাক্য একটা লিস্ট হিসেবে আউটপুটে আসবে। সেই লিস্টে শব্দগুলোকে ইন্টেজারে কনভার্ট হয়ে আসবে এখানে। এখানে উদাহরণে দেখি।\n",
    "\n",
    "পাশাপাশি প্যাডিং এর উদাহরণ দেখবো সামনে।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2gjCYVmSZTEW"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    'আমি ভালবাসি বই পড়তে,',\n",
    "    'আমি ভালবাসি বই লিখতে!',\n",
    "    'বইমেলা এলে আমি প্রচুর বই কিনি',\n",
    "    'এইবার বইমেলায় আমার সাথে তুমি কি যাবে?'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "UF00HP2MCe0z",
    "outputId": "6da960e9-577d-4323-8dda-3c6f0d2764ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ওয়ার্ড ইনডেক্স =  {'আমি': 1, 'বই': 2, 'ভালবাসি': 3, 'পড়তে': 4, 'লিখতে': 5, 'বইমেলা': 6, 'এলে': 7, 'প্রচুর': 8, 'কিনি': 9, 'এইবার': 10, 'বইমেলায়': 11, 'আমার': 12, 'সাথে': 13, 'তুমি': 14, 'কি': 15, 'যাবে': 16}\n",
      "\n",
      "সিকোয়েন্স =  [[1, 3, 2, 4], [1, 3, 2, 5], [6, 7, 1, 8, 2, 9], [10, 11, 12, 13, 14, 15, 16]]\n"
     ]
    }
   ],
   "source": [
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"\\nওয়ার্ড ইনডেক্স = \" , word_index)\n",
    "print(\"\\nসিকোয়েন্স = \" , sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FYD2KJfqU57N"
   },
   "source": [
    "টোকেনাইজারের texts_to_sequences মেথড আগের এবং পরের সব শব্দগুলোকে যোগ করেছে নতুন ওয়ার্ড এবং সংখ্যা ভ্যালু জোড়া ডিকশনারিতে। এখানে নতুন দুটো ৭ এবং ৮ শব্দের বাক্য যোগ করেছি কর্পাসে।\n",
    "\n",
    "![alt text](https://github.com/raqueeb/nlp_bangla/raw/master/assets/index2.png)\n",
    "\n",
    "চিত্র: ওয়ার্ড ইনডেক্স এবং সিকোয়েন্সের যোগসুত্র\n",
    "\n",
    "আমাদের নতুন কর্পাসে আগের যেই শব্দগুলো ছিল তার সাথে নতুন দুটো বাক্যের শুধুমাত্র নতুন শব্দগুলো যোগ হয়েছে পরে। আমাদের এখানে ওয়ার্ড ইন্ডেক্স এবং সিক্যুয়েন্স এই দুটোর মধ্যে সংযোগ সূত্র হিসেবে যে শব্দগুলো বারবার ব্যবহার করা হয়েছে সেগুলো চলে গেছে উপরে।  এর পাশাপাশি নতুন বাক্য থেকে যে শব্দগুলো নতুন এসেছে সেগুলো এসে যোগ হয়েছে ডিকশনারিতে নিচের দিকে।  এখানে একটা কথা বলে রাখা ভালো আমাদের চারপাশে আমরা যে শব্দ গুলোকে ব্যবহার করছি সেখানে মেশিন লার্নিং শুধুমাত্র এই কর্পাস এর ভেতরের শব্দ গুলোকে চিনে।  তার বাইরের কোনো শব্দ দিয়ে যদি আমরা টেস্ট করি তাহলে সেই শব্দগুলোকে সে চিনতে পারবে না।  এটাই তো স্বাভাবিক তাই না?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S26-02VuxFt0"
   },
   "source": [
    "## সব বাক্যকে এক কাতারে নিয়ে আসা (প্যাডিং)\n",
    "\n",
    "প্যাডিং নিয়ে উদাহরণ দিয়ে কথা বলি বরং। আমরা যখন কথা বলি তখন মেপে মেপে কি কথা বলা যায়? অথবা যখন একটা গল্প লিখছি, সেখানে বাক্যের মধ্যে শব্দ হচ্ছে একটা ভ্যারিয়েবল জিনিস। গুরুত্বের সাথে বাড়ে কমে। এদিকে নিউরাল নেটওয়ার্কের কাজ হচ্ছে সবকিছুই একই সাইজে তার পাওয়া চাই। ইমেজ নিয়ে কাজ করতে গিয়ে দেখেছি - যেই সাইজের ইমেজ হোক না কেন, সেটাকে একই পিক্সেলে নিয়ে আসতাম নিউরাল নেটওয়ার্কে ফিড করার আগে। প্রি-প্রসেসিংয়ে। \n",
    "\n",
    "ভাষা অবশ্যই একটা বিশেষায়িত জিনিস,  আর সে কারণেই আমাদেরকে এখানে বিশেষায়িত কাজ করতে হবে।  এরকম কি হতে পারে আমরা একটা বাক্যে ২০টা শব্দের বেশি ব্যবহার করি? আমার মনে হয় করি, বিশেষ করে যখন লিখি। মেশিন লার্নিং মডেলের জন্য আমরা ধরে নিতে পারি একটা বাক্যে ৮০টার বেশি শব্দ সচরাচর আসবেনা।\n",
    "\n",
    "সেখানে যেহেতু আমরা উদাহরণে মাত্র চারটা বাক্য নিয়ে আলাপ করছি, আমাদের দেখা মতে চারটা বাক্যের মধ্যে শেষের বাক্যে সর্বোচ্চ শব্দ সংখ্যা হচ্ছে ৭।  এখানে আমরা যদি বাক্যের সর্বোচ্চ সিকোয়েন্স ৭ দিয়ে দেই, তাহলে শব্দের ছোট বাক্যগুলোকে আমরা অন্য কিছু দিয়ে প্যাডিং মানে গোজামিল দিতে পারি। এরপর মেশিন ফেলে দেবে গোজামিল মানে বাড়তি প্যাডিং অংশ।\n",
    "\n",
    "## pad_sequences মেথডের ব্যবহার\n",
    "\n",
    "প্যাডিং নিয়ে কাজ করতে tensorflow.keras.preprocessing.sequence থেকে আমরা pad_sequences মেথডকে ইমপোর্ট করে নেব আমাদের বের করা সিকোয়েন্সকে প্যাডিং করে নিতে। এখানে দুটো আউটপুট দেখুন। শুরুতে শুধুমাত্র সিকোয়েন্স, এরপরে সেই সিকোয়েন্সের প্যাডিং। শব্দ সংখ্যা - যেহেতু এখানে সর্বোচ্চ মানে ৭টা শব্দ। সে স্বয়ংক্রিয়ভাবে ভেতরে ভেতরে একটা ম্যাক্সিমাম লেনথ ধরে নিয়েছে। আচ্ছা, কি দিয়ে প্যাডিং দেয়া যায়? '০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স বোঝা যাবে সহজে।\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "7_BugO0SxMZi",
    "outputId": "86cc3f1d-8f6c-47a3-fc90-fd402edd0f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ওয়ার্ড ইনডেক্স =  {'আমি': 1, 'বই': 2, 'ভালবাসি': 3, 'পড়তে': 4, 'লিখতে': 5, 'বইমেলা': 6, 'এলে': 7, 'প্রচুর': 8, 'কিনি': 9, 'এইবার': 10, 'বইমেলায়': 11, 'আমার': 12, 'সাথে': 13, 'তুমি': 14, 'কি': 15, 'যাবে': 16}\n",
      "\n",
      "সিকোয়েন্স =  [[1, 3, 2, 4], [1, 3, 2, 5], [6, 7, 1, 8, 2, 9], [10, 11, 12, 13, 14, 15, 16]]\n",
      "\n",
      "'০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স:\n",
      "[[ 0  0  0  1  3  2  4]\n",
      " [ 0  0  0  1  3  2  5]\n",
      " [ 0  6  7  1  8  2  9]\n",
      " [10 11 12 13 14 15 16]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded = pad_sequences(sequences)\n",
    "print(\"ওয়ার্ড ইনডেক্স = \" , word_index)\n",
    "print(\"\\nসিকোয়েন্স = \" , sequences)\n",
    "print(\"\\n'০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "46yJhK5aigVh"
   },
   "source": [
    "কেমন হয় আমরা ম্যাক্সিমাম একটা লেনথ বলে দেই? ধরে নেই সেটা maxlen=10, - কেমন হবে তখন? আর আমরা চাচ্ছি '০' বসবে পরে। কি করতে হবে? বলে দেই padding='post' মানে পরে বসবে। 'pre' নয়। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Quv-iiPkfMBV",
    "outputId": "6b233a1b-7bbd-4efe-cb16-8fdd27474fab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স:\n",
      "[[ 1  3  2  4  0  0  0  0  0  0]\n",
      " [ 1  3  2  5  0  0  0  0  0  0]\n",
      " [ 6  7  1  8  2  9  0  0  0  0]\n",
      " [10 11 12 13 14 15 16  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', maxlen=10)\n",
    "\n",
    "print(\"'০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vU_yTBevNLZE"
   },
   "source": [
    "আগের উদাহরন থেকে নেয়া। ছোট বাক্যগুলোর শুরুতে ০। এর মানে নিউরাল নেটওয়ার্ককে ফাঁকি দেয়া গেছে।\n",
    "\n",
    "![চিত্র: সংখ্যা](https://github.com/raqueeb/nlp_bangla/raw/master/assets/t2seq.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Wgk1qcPlyjo"
   },
   "source": [
    "কেমন হয় আমাদের কর্পাসে এতো বেশি বেশি শব্দের বাক্য থাকে? আমার দেখা মতে একেকটা বই, মুভি রিভিউতে ভেতরে 'স্টপওয়ার্ড' সহ ৮০ এর বেশি শব্দ থাকলেও শুরুর ৮০ শব্দে ভালো ধারণা আসে। তখন যদি আমরা শুরুর ৮০টা শব্দ নিতে চাই? এখানে উদাহরণে ম্যাক্সিমাম লেনথ ৫ দিয়ে দেখি। আমরা 'ট্রান্কেট' (truncating='post') করবো এখানে। শেষের বাক্যের ৭টা শব্দের মধ্যে শেষের ২টা শব্দ কেটে গিয়েছে। সিকোয়েন্স ১৫ এবং ১৬।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "ty74HURdQUQy",
    "outputId": "fb35f943-a200-4c68-9395-026e17ded6b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স তবে ৫টা সর্বোচ্চ শব্দ:\n",
      "[[ 1  3  2  4  0]\n",
      " [ 1  3  2  5  0]\n",
      " [ 6  7  1  8  2]\n",
      " [10 11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "padded = pad_sequences(sequences, padding='post', truncating='post', maxlen=5)\n",
    "\n",
    "print(\"'০' দিয়ে প্যাডিং দেয়া সিকোয়েন্স তবে ৫টা সর্বোচ্চ শব্দ:\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0mevgVaUhRn"
   },
   "source": [
    "## কি হবে, যখন ওয়ার্ড ইনডেক্সে শব্দটা থাকবে না?\n",
    "\n",
    "আমরা যদি একটা নিউরাল নেটওয়ার্ক কে একটা নির্দিষ্ট কর পাশের টেক্সট মানে শব্দ এবং তার সেখান থেকে জেনারেটেড ওয়ার্ড ইন্ডেক্স  কে ট্রেইন করি তাহলে আমরা সেই ট্রেনিং মডেল থেকে একটা ইনফারেন্ট ব্যবহার করতে পারব।  এর অর্থ হচ্ছে আমাদেরকে সেই একই ওয়ার্ড ইন্ডেক্স এর উপর ইনফার করতে হবে যাকে আমরা সেই একই শব্দ দিয়ে ট্রেনিং করিয়েছিলাম।এর ব্যত্যয় হলে আমাদের কাজের কোন আউটপুট থাকবে না।  আমরা যে কর পাশ দিয়ে নিউরাল নেটওয়ার্ককে  ট্রেইন করাবো এর বাহিরে যদি কোন শব্দ আসে টেস্ট ডাটা সেটে তাহলে তো সে সেটার আউটকামস দিতে পারবে না। নিচে আমাদের নতুন দুটো বাক্য স্টেশন হিসেবে চালিয়ে দেখি।  আপনারা দেখতে পাচ্ছেন এখানে কিছু কিছু শব্দ আমাদের কর পাশে আছে তবে কিছু শব্দ আমাদের আগের কর্পাস এবং ওয়ার্ড ইনডেক্সে ছিলনা,  তাহলে এখন কি হবে?এখানে আমাদের ওয়ার্ড ইনডেক্স এবং সিক্যুয়েন্স দুটো দেখি।  আপনি বলুন এখানে কি কি মিসিং আছে অথবা কেন মিসিং আছে? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5At652ZlU-RQ",
    "outputId": "90cb532d-11cf-43e3-9f49-4e56afe9f9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "টেস্ট সিকোয়েন্স =  [[1, 3, 2, 4], [11, 8, 2]]\n"
     ]
    }
   ],
   "source": [
    "# আমরা নতুন কিছু শব্দ ব্যবহার করি যেটা আমাদের টোকেনাইজারকে ফিট করা হয়নি\n",
    "test_data = [\n",
    "    'আমি আসলেই ভালবাসি বই পড়তে',\n",
    "    'বইমেলায় এবার প্রচুর নতুন বই এসেছে!'\n",
    "]\n",
    "\n",
    "# texts_to_sequences কি বের করে দেখি\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(\"টেস্ট সিকোয়েন্স = \", test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05SWR3C7WSbs"
   },
   "source": [
    "ওমা, কি হলো ব্যাপারটা? কিছু কিছু শব্দ হারিয়ে গেছে ওয়ার্ড ইনডেক্স এ না থাকার কারণে। কেন হলো এটা?\n",
    "\n",
    "![alt text](https://github.com/raqueeb/nlp_bangla/raw/master/assets/index3.png)\n",
    "\n",
    "এর অর্থ হচ্ছে (অনেক শব্দ মিসিং, কর্পাসে নেই)\n",
    "\n",
    "টেস্ট সিকোয়েন্স =  [[1, 3, 2, 4], [11, 8, 2]]\n",
    "\n",
    "টেস্ট সিকোয়েন্স =  [[আমি, ভালবাসি, বই, পড়তে], [বইমেলায়, প্রচুর, বই]]\n",
    "\n",
    "১. প্রথম বাক্য: 'আমি ভালবাসি বই পড়তে', **আসলেই** মিসিং\n",
    "\n",
    "২. পরের বাক্যে: 'বইমেলায় প্রচুর বই', **এবার, নতুন, এসেছে** মিসিং।\n",
    "\n",
    "এটা থেকে বাঁচার উপায় কি? তারমানে একটা ভাষায় সব শব্দকে আনতে হবে আমাদের ওয়ার্ড ইনডেক্সে? একটা উপায়, বাংলা সব শব্দকে আনতে হবে কর্পাসে। তাই বলে কি সব শব্দ আনা যাবে? কেমন হবে যখন কেউ 'বাংলিশ' বলবে? অথবা একদমই নতুন শব্দ?\n",
    "\n",
    "কি হবে তখন?\n",
    "\n",
    "এভাবে সম্ভব?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNrHuJaypm3ah6fsoUBEd5u",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "text_sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
