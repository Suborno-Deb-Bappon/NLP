{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/raqueeb/nlp_bangla/blob/master/Chap_2_NLP_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1s9CnVh9eO2"
   },
   "source": [
    "## টেক্সট প্রি-প্রসেসিং\n",
    "\n",
    "আমরা এর আগে হাতে-কলমে যেই কাজগুলো করলাম, সেটা ডাটা প্রি-প্রসেসিংয়ের অংশ। আমাদের বইগুলোতে বড় বাক্য থাকলে সেগুলোকে ভেঙে ছোট করে শব্দের টোকেন বানানোর যে প্রক্রিয়া সেটা এই ডাটা প্রি-প্রসেসিং এর অংশ। বাক্যের প্রতিটা শব্দকে ‘কর্পাসে’ ঢোকানোর জন্য শব্দের সিকোয়েন্স অর্থাৎ সেই শব্দটাকে প্রথমে টোকেন তৈরি করে সেটা থেকে সংখ্যার সিকোয়েন্সের জন্য যে কাজগুলো করতে হয় সেগুলো মডেল বিল্ডিং অথবা ডাটা এক্সপ্লোরেশন করার আগে ডাটা প্রি-প্রসেসিং অংশে করতে হবে। ডাটার ভেতরে যদি কোনো ‘নয়েজ’ থাকে, যেমন টেক্সট ফাইল এর জন্য হেডার, ফুটার অথবা ‘এইচটিএমএল’ বা ‘এক্সএমএল’, বিভিন্ন মার্কআপ, মেটাডাটা থাকলে সেগুলোকে ফেলে দেবো এই প্রি-প্রসেসিং এর অংশে। ফাইল যদি অন্য কোন ফরম্যাটে থাকে, যেমন ‘জেসন’ - সেই কনভার্শনগুলো প্রি-প্রসেসিং এর অংশ।\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/raqueeb/nlp_bangla/master/assets/pre.png)\n",
    "\n",
    "ডাটার নরমালাইজেশন যেমন, ‘স্টেমিং’ এবং ‘লেমাটাইজেশন’, এগুলো বলে আপনাদের মাথা খারাপ করতে চাই না এই মুহূর্তে। মনে রাখতে হবে, টোকেনাইজেশনের পরে আমরা আর অক্ষরের পর্যায়ে থাকছিনা, সরাসরি চলে যাচ্ছি শব্দের পর্যায় - যেখানে এধরনের কিছু ‘নরমালাইজেশন’ করা যেতে পারে।\n",
    "এখানের উদাহরণগুলো একটু ভালো করে লক্ষ্য করুন। এই ব্যাপারগুলো নিয়ে আমরা সামনে আরো বিশদ করে আলাপ করব তবে এখানে ‘পয়েন্ট বাই পয়েন্ট’ ব্যাপারটা বোঝার চেষ্টা করুন।  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zaCMcjMQifQc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'আমি বই ভালবাসি',\n",
    "    'আমি বই পড়তে ভালবাসি!',\n",
    "    ]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "W1jvtYj1mRvS",
    "outputId": "2b7d134c-7b0e-4b8e-d1c6-a74d35c6cbd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xI7YHfi7Bxtt"
   },
   "source": [
    "## ভোকাবুলারি,কর্পাস, ডিকশনারি\n",
    "\n",
    "এখানে fit_on_texts নিজের ভেতরের যে ভোকাবুলারি তৈরি থাকার কথা সেটাকে আপডেট করে নেয় এর মধ্যে যতগুলো শব্দ লিস্ট আকারে থাকে। আমরা যদি বলি “আমি যাই” সেটাকে একটা ডিকশনারি বানিয়ে দিবে এভাবে শব্দ দিয়ে। word_index[\"আমি\"] = 1; word_index[\"যাই\"] = 2, এভাবে চলতে থাকবে আরো যতো শব্দ থাকে। এটাকে আমরা শব্দের একটা ডিকশনারি বলতে পারি যেখানে প্রতিটা শব্দ একটা করে ইউনিক সংখ্যা (১,২,৩ …) পায়। এখানে শূন্যকে আলাদা করে রাখা হয়েছে প্যাডিংয়ের জন্য। তবে, সংখ্যার মান যত কম থাকবে ততবেশি সেই শব্দটা ‘কর্পাসে’ ব্যবহার হবে। এখানে ‘কর্পাস’ হচ্ছে শব্দের ডিকশনারি। তবে, আমাদের অভিজ্ঞতা বলে কিছু কিছু ‘স্টপ ওয়ার্ড’ বারবার ব্যবহার হয় বলে সেগুলোই আগে চলে আসতে পারে। সেজন্য আমরা অনেক সময় ‘স্টপ ওয়ার্ড’ ফেলে দিতে চাই।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "mdNSi7gOm-3K",
    "outputId": "cb2cb6bc-fdca-42c4-9b6b-f8e2cde521ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'আমি': 1, 'বই': 2, 'ভালবাসি': 3, 'পড়তে': 4}\n"
     ]
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yn9fr2RjB-KZ"
   },
   "source": [
    "আমাদের এই ডিকশনারিতে কি কি শব্দ আছে সেটাকে ক্রমানুসারে দেখি। তবে এখানে আমরা প্রতিটা শব্দ কয়বার করে আছে - সেটা দেখতে পারি।"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bnReI1YflIyT",
    "outputId": "ea5c1775-e7b8-4e29-9a54-02ef42dbb94a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('আমি', 2), ('বই', 2), ('ভালবাসি', 2), ('পড়তে', 1)])"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kk1qZjzEDUsO"
   },
   "source": [
    "## ‘টোকেনাইজারে’র বর্তমান ‘কনফিগারেশন’ কি আছে জানতে এই ফাংশন\n",
    "\n",
    "অনেক সময় ‘টোকেনাইজারে’র বর্তমান ‘কনফিগারেশন’ কি আছে - সেটা দেখার জন্য এই ফাংশনটা ব্যবহার করা যেতে পারে,  বিশেষ করে যখন কিছু কিছু ক্ষেত্রে টোকেনাইজার কেন এই কাজটা করল অথবা এই জিনিসটা কেন এলো - সেটা বোঝার জন্য এই ফাংশনটা খুবই জরুরী। "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "ry6gfPTglrHM",
    "outputId": "5ef13036-dc52-4783-e17d-54aa906ffdc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char_level': False,\n",
       " 'document_count': 2,\n",
       " 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
       " 'index_docs': '{\"2\": 2, \"1\": 2, \"3\": 2, \"4\": 1}',\n",
       " 'index_word': '{\"1\": \"\\\\u0986\\\\u09ae\\\\u09bf\", \"2\": \"\\\\u09ac\\\\u0987\", \"3\": \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\", \"4\": \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\"}',\n",
       " 'lower': True,\n",
       " 'num_words': 10,\n",
       " 'oov_token': None,\n",
       " 'split': ' ',\n",
       " 'word_counts': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 2, \"\\\\u09ac\\\\u0987\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 1}',\n",
       " 'word_docs': '{\"\\\\u09ac\\\\u0987\": 2, \"\\\\u0986\\\\u09ae\\\\u09bf\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 2, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 1}',\n",
       " 'word_index': '{\"\\\\u0986\\\\u09ae\\\\u09bf\": 1, \"\\\\u09ac\\\\u0987\": 2, \"\\\\u09ad\\\\u09be\\\\u09b2\\\\u09ac\\\\u09be\\\\u09b8\\\\u09bf\": 3, \"\\\\u09aa\\\\u09dc\\\\u09a4\\\\u09c7\": 4}'}"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_config()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "NLP_tokenization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
